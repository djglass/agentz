# LocalStack Tofu LLM API

This repo contains a fully local, serverless API stack powered by:

- âœ… [LocalStack](https://localstack.cloud/) (emulated AWS services)
- âœ… [OpenTofu](https://opentofu.org/) (open-source Terraform)
- âœ… [AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html) (locally deployed)
- âœ… [Ollama](https://ollama.com) (serving Llama3 or any local LLM)

### ðŸ”§ Setup

1. Start [Ollama](https://ollama.com) and pull a model:

   ```bash
   ollama serve &
   ollama pull llama3

